% Evaluation of Irish Rules model

# Introduction 

## Libraries

```{r opts, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "images/"
)
```

Packages needed for the analysis are loaded. If the libraries do not exist locally, they will be downloaded.

```{r setup, message=FALSE, warning=FALSE}
list.of.packages <-
  c(
    "tidyverse",
    "readxl",
    "data.table",
    "knitr",
    "zoo",
    "imputeTS",
    "padr",
    "devtools",
    "pracma",
    "remotes",
    "parallel",
    "pbapply",
    "ggrepel",
    "ggthemes",
    "egg",
    "rsm",
    "GGally",
    "R.utils",
    "rcompanion",
    "mgsub",
    "here",
    "stringr",
    "pander"
  )

new.packages <-
  list.of.packages[!(list.of.packages %in% installed.packages()[, "Package"])]

#Download packages that are not already present
if (length(new.packages))
  install.packages(new.packages)

if ("gt" %in% installed.packages() == FALSE)
  remotes::install_github("rstudio/gt")

list.of.packages <- c(list.of.packages, "gt")
packages_load <-
  lapply(list.of.packages, require, character.only = TRUE)

#Print warning if there is a problem with installing/loading some of packages
if (any(as.numeric(packages_load) == 0)) {
  warning(paste("Package/s", paste(list.of.packages[packages_load != TRUE]), "not loaded!"))
} else {
  print("All packages were successfully loaded.")
}
rm(list.of.packages, new.packages, packages_load)
```

## Reproducibility
Outputs of computational exhaustive procedures have been stored with the data and can be loaded directly.  

# Data 

## Weather Data
Historical weather data from Met Éireann  synoptic weather station at Oak Park was used for model evaluation. The trial sites were in the radius of up to 500 m from the station in all years. 

```{r weather-data, message=FALSE, warning=FALSE,fig.align='center'}
#Weather data, parameters and cut off dates
load(file = here("data", "op_2007_16", "OP_2007-2016.RData"))

OP[1:5, 1:20] %>% gt()
```

Additional variables needed for the analysis.
```{r add-vars-OP}
colnames(OP)[which(names(OP) == "year")] <- "year_var"
OP <-
  add_column(OP, week_var = data.table::week(OP$date), .before = "i_rain")
OP <-
  add_column(OP, doy = data.table::yday(OP$date), .before = "i_rain")
```
Subset the data to exclude the months of the year which we do not need for the analysis. 
```{r subset-OP}
OP <- subset(OP, month > 3 & month < 10)
```
Remove the variables we don't need for the analysis, to make some speed gains. 

Get a summary of missing values for the variables of interest. 
```{r summarise-missing-OP}
OP %>% group_by(year_var)  %>%
  summarize(
    NA_rain = sum(is.na(rain)),
    NA_temp = sum(is.na(temp)),
    NA_rhum = sum(is.na(rhum))
  )
```


Missing value imputation with qbic spline function works well up to 8 consecutive values, for variables that have some seasonal frequency, temperature and relative humidity in our case. 

```{r impute-missing, message=FALSE, warning=FALSE}
infil_gap <- 8 #Maximum length of the infill gap
OP$temp <-
  round(na.spline(OP$temp, na.rm = FALSE, maxgap = infil_gap), 1)
OP$rhum <-
  round(na.spline(OP$rhum, na.rm = FALSE, maxgap = infil_gap), 0)
OP$rhum  <- sapply(OP$rhum, function(x)
  ifelse(x > 100, x <- 100, x))
#Check if the imputation worked
OP %>% group_by(year_var)  %>%
  summarize(
    NA_rain = sum(is.na(rain)),
    NA_temp = sum(is.na(temp)),
    NA_rhum = sum(is.na(rhum))
  )
```

Rain is somewhat harder to impute but there are ways around this problem, especially when there are only a few values missing. Since rain data is required only in certain rare situations for the model to run, defined within the model, we can use the same conditions to impute missing values outside of that range. We are certain that rain is irrelevant if relative humidity is below 88&nbsp;% and temperature of 8&nbsp;C, and these values can then be replaced with 0. Tis way we will know if rain data is missing in areas of interest.

```{r imput-rain}
OP[is.na(OP$rain), ]$rain <-
  with(OP[is.na(OP$rain), ], ifelse(rhum < 88 | temp < 8, 0, rain))
OP %>% group_by(year_var)  %>%
  summarize(
    NA_rain = sum(is.na(rain)),
    NA_temp = sum(is.na(temp)),
    NA_rhum = sum(is.na(rhum))
  )
```
We have no missing values and it is safe to proceed.   

## Bio data
Planting date and first observation of the disease are loaded. Emergence takes up to 3 weeks under Irish conditions. Period when healthy host present from emergence until 14 days prior to a first observation of the disease in the field. Warning period 10-day ‘warning period’ considered to last from -14 days to – 4 days prior to disease observed in the field. The 4-day period was assumed to be a minimum time needed from incubation period, for establishment of visible disease symptoms in the field.

```{r epidemic-initiation, fig.align='center'}
#Get subsets of data for period before the epidemics were initiated
dates_cut <-
  read_csv(
    here("data", "op_2007_16", "plantingdates.csv"),
    col_types = cols(
      disease_observed = col_date(format = "%d/%m/%Y"),
      last_assessment = col_date(format = "%d/%m/%Y")
    )
  )
dates_cut$planting_date <-
  as.Date(dates_cut$planting_date, format = "%d/%m/%Y")
dates_cut$emergence <- as.Date(dates_cut$planting_date) + 21
#st warnning period to 14 days before disease onset
dates_cut <- add_column(dates_cut, disease_onset = as.Date(dates_cut$disease_observed) - 4,
                        .before = "disease_observed")

dates_cut <- add_column(dates_cut, warning = as.Date(dates_cut$disease_onset) - 10, .before = "disease_onset" )
rownames(dates_cut) <- NULL

dates_cut %>% rename_all(. %>% capitalize() %>% gsub("_", " ", .))
```

## The Model

Implementation of the model

```{r irish-model}
IrishRulesModel <- function(weather,
                            param = NULL,
                            infill_gap = NULL) {

  # wetness requirement prior to infection accumulation start
  # time window of 6 hours, 3 before/after sporulation ends
  wet_before <- 3
  wet_after <- 3
  
  # Parameter list
  if (is.null(param)) {
    rh_thresh <- 90
    temp_thresh <- 10
    hours <- 12   #sum of hours before EBH accumulation
  } else {
    #pass a vector of parameters
    rh_thresh <- as.numeric(param[2])
    temp_thresh <- as.numeric(param[3])
    hours <- as.numeric(param[4])
    lw_rhum <-
      param[5]           #if is NA then only rain data will be used
  }
  
  #threshold for estimation of leaf wetness using relative humidity
  lw_rhum_threshold <- 90
    
  weather[["rain"]] -> rain
  if ("rhum" %in% names(weather)) {
    weather[["rhum"]] -> rh
  }
  if ("rh" %in% names(weather)) {
    weather[["rh"]] -> rh
  }
  weather[["temp"]] -> temp
  
  # This function to infil missing values to let the model run
  #If maximum infill gap is not provided it is defaulted to 7
  if (is.null(infill_gap)) {
    infill_gap <- 7
  }
  
  if (sum(is.na(with(weather, rain, temp, rhum))) > 0) {
    temp <-
      round(zoo::na.spline(temp, na.rm = FALSE, maxgap = infill_gap), 1)
    rh <-
      round(zoo::na.spline(rh, na.rm = FALSE, maxgap = infill_gap), 0)
    rh  <- sapply(rh, function(x) ifelse(x > 100, x <- 100, x))
  }
  
  if (sum(is.na(with(weather, rain, temp, rhum))) > 0) {
    stop(print("The sum of NAs is more than 7! Check your weather data."))
  }
  
  #"Out of boounds"
  rain <- c(rain, rep(0, 20))
  temp <- c(temp, rep(0, 20))
  rh <- c(rh, rep(0, 20))
  
  # conditions for sporulation
  criteria <- as.numeric(temp >= temp_thresh & rh >= rh_thresh)
  
  #cumulative sum of hours that meet the criteria for sporulatoion with restart at zero
  criteria_sum <-
    stats::ave(criteria, cumsum(criteria == 0), FUN = cumsum)
  
  #Initiate risk accumulation vector
  risk <- rep(0, length(temp))
  
  criteria_met12  <-
    as.numeric(criteria_sum >= hours) #accumulation of EBH starts after sporulation
  idx             <- which(criteria_sum == hours)
  
  
  #If there are no accumulations return vector with zeros
  if (sum(criteria_sum == hours) == 0) {
    #breaks the loop if there is no initial accumulation of 12 hours
    head(risk, -20)
  } else{
    for (j in 1:length(idx)) {
      #switch that looks if there was wetness: first rain, then both rain and rh, if rh exists
      if (if (lw_rhum=="rain") {
        #if only rain
        (sum(rain[(idx[j] - wet_before):(idx[j] + wet_after)]) >= 0.1)           #just see rain sum
      } else{
        any((any(rh[(idx[j] - wet_before):(idx[j] + wet_after)] >= lw_rhum_threshold)) |
            #take both as possible switches
            (sum(rain[(idx[j] - wet_before):(idx[j] + wet_after)]) >= 0.1))
      })
        # outputs true or false
      {
        n <- idx[j]        #start accumulation from 12th hour
      } else {
        n <- idx[j] + 4      #start accumulation from 16th hour
      }
      s <- criteria_met12[n]
      # if a break of less than or equal to 5 hours
      m <- n - 1
      
      while (s == 1)
      {
        risk[n] <- risk[m] + 1
        n <- n + 1
        
        m <- n - 1
        
        s <- criteria[n]
        if (s == 0 && (criteria[n + 2] == 1)) {
          n = n + 2
          
          s = 1
          
        } else if (s == 0 && (criteria[n + 3] == 1)) {
          n = n + 3
          
          s = 1
          
        } else if (s == 0 && (criteria[n + 4] == 1)) {
          n = n + 4
          
          s = 1
          
        } else if (s == 0 && (criteria[n + 5] == 1)) {
          n = n + 5
          
          s = 1
          
        }
      }
      
    }
    head(risk, -20) #remove last 20 values that were added to vectors to prevent "Out of bounds" issue
    
  }
  
}
```

# The Analysis 

The set of the most important variables of Irish rules model is evaluated. The Excel sheet with parameters under evaluation is available in data folder and can be changed and used for model evaluation in other locations. Column named `90_10_12_rain` represents set of the original model parameters. 

```{r load-parameters, message=FALSE, warning=FALSE, fig.align='center'}
#read in parameters
parameters <-
  read_excel(here("data", "op_2007_16", "par.xlsx"), sheet = "par")
parameters %>% gt()
```


```{r fig.align='center'}
params <- expand.grid(parameters[, 1:3])

#set the leaf wetness threshold to NA, meaning only rain is considered as an estimator for leaf wetness, as in original model
params$lw_rh <- "rain"

#Repeat all of the analysis considering rh >= 90% and rain as an estimator of leaf wetness
params2 <- params
params2$lw_rh <- as.character("rainrh")

parameters <- bind_rows(params, params2)
rm(params, params2)

# set a column with a name for each model
parameters <- add_column(parameters, model = NA, .before = 1)
for (i in seq_along(1:nrow(parameters))) {
  parameters[i, 1] <-
    paste0(parameters[i, 2:length(names(parameters))], collapse = "_")
}
str(parameters)
```

## Model Run

The model has been run with each set of parameters, and columns with model outputs are attached to weather data frame. Names of new columns correspond to the set of parameters supplied. 

```{r run-model, eval=FALSE}
# This chunk is not run
for(i in 1:nrow(parameters)) {
  loop_var <- apply(parameters[i, ], 1, function(x) {
    #run the model with different parameters
    k <-
      lapply(split(OP, factor(OP$year)), function(chunk)
        IrishRulesModel(chunk, x)) #get the list of outputs
    unlist(k) -> k  #make it a vector
  })
  OP[, ncol(OP) + 1] <-  as.numeric(loop_var)
  rm(loop_var)
  names(OP)[ncol(OP)] <- paste0(parameters[i, 1])
  print(paste(i, "of", nrow(parameters)))
}


nn <- paste0(names(OP[, 26:length(names(OP))]), "_ebh", "")
setnames(OP, old = c(names(OP[, 26:length(names(OP))])), new = nn)
rm( nn)
```

Instead, just load the output from a prior model run to save time.

```{r load-model-run, fig.align='center'}
load(file = here("data", "op_2007_16", "OP_for_analysis.RData"))
# Sample of outputs
head(OP[, c(1, 7, 9, 15, 24:30)], 5)
```

## Evaluation procedure

The function `SensParametersCalc` calculates Sensitivity and Specificity of each variation of the model.

```{r sens-spec-calc}
SensParametersCalc <-
  function(y,
           weather_data,
           dates_cut,
           prot_duration = NULL) {
    #Set the warning threshold and run the rest of the script
    warning_threshold <- y
    #data
    fun_df <- weather_data
    
    #A function to subset the data for the period of interest in each year
    test.overlap = function(vals, start_date, end_date) {
      rowSums(mapply(function(a, b)
        between(vals, a, b),
        start_date, end_date)) > 0
    }
    
    #Subset  each year from emergence to disease onset and calculate number of FP and TN
    fptn_df <-
      fun_df  %>%
      #Subset the of the data for the duration of non-warning period for each year
      filter(test.overlap(short_date, dates_cut$emergence, dates_cut$warning)) %>%
      select(
        ends_with("year_var"),
        ends_with("week_var"),
        ends_with("doy"),
        ends_with("_ebh")
      ) %>%
      group_by(year_var) %>%
      #if there was an accumulation from previous day, it would triger a warning
      #Check all of the first five rows because of possible break of 5 hours
      mutate_at(., .vars = colnames(.[grep("ebh", colnames(.))]),
                funs(
                  ifelse(row_number() <= 5 & . >= warning_threshold,
                         warning_threshold, .)
                )) %>%
      #all five values all changed so we have to delete 4 of them and leave only 1
      mutate_at(., .vars = colnames(.[grep("ebh", colnames(.))]),
                funs(ifelse(
                  row_number() <= 4 & . == warning_threshold, 0, .
                ))) %>%
      # Change values coresponding to the warning threshold to 1 for calculating the sum
      mutate_at(., .vars = colnames(.[grep("ebh", colnames(.))]),
                funs(ifelse(. == warning_threshold, 1, 0))) %>%
      group_by(year_var, week_var, doy) %>%
      summarise_at(., .vars = colnames(.[grep("ebh", colnames(.))]), .funs = sum)
    
    
    #Each warning would cause treatment that will keep the plants protected for a period of time
    prot_duration <-
      ifelse(is.null(prot_duration), 7, prot_duration)#If not defined default value is 7 days
    
    TreatmentWindow <- function(x, prot_duration) {
      # don't leave commented code. Use or remove
      # x <- fptn_df[["93_10_12_rain_ebh"]]
      y <-
        vector(mode = "numeric", length = length(x) + prot_duration)
      for (i in seq_along(x)) {
        if (x[i] == 1) {
          y[i:c(i + prot_duration)] <- 1
        }
      }
      y
    }
    fptn_df[grep("ebh", colnames(fptn_df))] <-
      lapply(fptn_df[grep("ebh", colnames(fptn_df))], function(x)
        TreatmentWindow(x, prot_duration))
    
    
    FP <-
      summarise_all(fptn_df[, colnames(fptn_df[, grep("ebh", colnames(fptn_df))])], .funs = sum)
    
    #Each warning will cause a treatment
    total_days <-
      nrow(fptn_df) #total  duration of non_warning period
    
    TN <- total_days - FP
    
    
    ##########################################################
    #subset for 10 days prior to disease onset: Warning period
    tpfn_df <-
      fun_df %>%
      #Subset of the data for the duration WARNING period in each year
      filter(test.overlap(short_date,  dates_cut$warning, dates_cut$disease_onset)) %>%
      select(
        ends_with("year_var"),
        ends_with("week_var"),
        ends_with("doy"),
        ends_with("_ebh")
      ) %>%
      mutate_at(., .vars = colnames(.[grep("ebh", colnames(.))]),
                funs(
                  ifelse(row_number() <= 5 &
                           . >= warning_threshold, warning_threshold, .)
                )) %>%
      #all five values is changed so we have to delete 4 of them and leave only one
      mutate_at(., .vars = colnames(.[grep("ebh", colnames(.))]),
                funs(ifelse(
                  row_number() <= 4 & . == warning_threshold,
                  0, .
                ))) %>%
      mutate_at(., .vars = colnames(.[grep("ebh", colnames(.))]),
                funs(ifelse(. == warning_threshold, 1, 0))) %>%
      group_by(year_var) %>%
      summarise_at(., .vars = colnames(.[, c(4:length(colnames(.)))]) , .funs = sum)  %>%
      mutate_at(., #some years have two warnings during the warning period
                .vars = colnames(.[grep("ebh", colnames(.))]), funs(ifelse(. >= 1 , 1, 0)))
    
    # tpfn_df[,1:4]
    
    TP <-
      summarise_all(tpfn_df[, colnames(tpfn_df[, grep("ebh", colnames(tpfn_df))])], .funs = sum)
    
    #number of outbreaks(in this case there is only one location, so number of outbreaks is same as number of years.
    unique(tpfn_df$year_var) %>%
      length() -> no_of_outbreaks
    FN <- no_of_outbreaks - TP
    
    #summary
    test <- data.frame(
      model = names(FP),
      FP = t(FP[1,]),
      tn = t(TN[1,]),
      TP = t(TP[1,]),
      FN = t(FN[1,])
    )
    names(test) <- c("model", "FP", "TN", "TP", "FN")
    test <- data.frame(test, row.names = NULL)
    
    
    test$model <-  str_replace(test$model, "_ebh", "")
    test$sens <- with(test, TP / (TP + FN))  #PTP sensitivity
    test$spec <- with(test, TN / (TN + FP))  #PTN specificity
    return_df <- test[, c("model", "sens")]
    return_df$"one_min_spec" <- 1 - test[, "spec"]
    return_df$cut_point <- warning_threshold
    
    return(return_df)
  }
```

This function was applied to output of each variation of the model with varying warning threshold from 1 to 18 EBH. The function is run with parallel processing support because it reduces the run time to 4 minutes with 4 cores i7(7th generation) and 12GB RAM laptop.

```{r apply-sense-spec-calc, eval=FALSE}
# this chunk not evaluated
#select max warning threshold
warning_thresholds <- 1:18
begin <- Sys.time()
#Detect the number of cores and set it to total minus 1, if there are multiple cores, to avoid overload
cores <- ifelse(detectCores() > 1, detectCores()-1, 1) 
cl <- makeCluster(cores)
clusterExport(cl, c("OP", "dates_cut", "SensParametersCalc"))
clusterEvalQ(cl, library("tidyverse"))
  ROC <- pbapply::pblapply(warning_thresholds, function(x)
  {
    xx <- SensParametersCalc(x,OP, dates_cut, prot_duration = 7)
    return(xx)
  },
  cl = cl
  )
begin- Sys.time() #check the duration of the process in the console
stopCluster(cl)
```

Rather, the results of calculations from a prior run can be directly loaded.

```{r load-ROC}
load(file = here("data", "op_2007_16","results","ROC_output.Rdata"))
```

Sort the outputs for each model variation.
```{r sort-ROC, fig.align='center'}
ROC_data <-
  as.data.frame(data.table::rbindlist(ROC))
rownames(ROC_data) <- NULL
# rm(ROC) #Outputs stored for each warning threshold/cutoffs
ROC_data <-
  base::split(ROC_data, ROC_data$model) #Get the list of all model outputs with different cutoffs
ROC_data[[1]] %>% gt()
```

## ROC

Contingency tables were created with sensitivity and specificity values from confusion matrix for each decision threshold for all model outputs from 1 to 18 EBH accumulation. Area under the curve (AUROC) was calculated using trapezoidal rule for each variation of the model outputs. 

```{r calculate-AUROC}
#function to calculate AUROC for list of inputs
GetAUC <- function(fun_df) {
  fun_df <- fun_df[rev(order(fun_df$cut_point)), ]
  auc <-
    pracma::trapz(c(0, fun_df$one_min_spec, 1), c(0, fun_df$sens, 1))
  result <- data.frame(model = unique(fun_df$model),
                       auc = auc)
  return(result)
}
AUROC_data <- lapply(ROC_data, function(x)
  GetAUC(x))
AUROC_data <-
  lapply(AUROC_data, function(x)
    mutate_if(x, is.factor, as.character))
AUROC_data <- bind_rows(AUROC_data)
```

Empirical ROC curve was created for each variation of the model.  

```{r plotROC, message=FALSE, warning=FALSE,out.width = '80%', out.height= '80%', fig.align="center"}
PlotROC <- function(df, numbering = NULL) {
  df <- df[rev(df$cut_point), ]
#append rows for plotting
x <- rep(NA, ncol(df))
df <- rbind(x, df)
df[nrow(df) + 1, ] <- NA
df$model <- unique(df$model[!is.na(df$model)])
df[1, c("sens", "one_min_spec")] <- 0
df[nrow(df), c("sens", "one_min_spec")] <- 1

#Condense labels for a single cutoff point
 df <- 
  df %>% 
  group_by(one_min_spec, sens, model) %>% 
  summarise(cut_point = ifelse(all(is.na(cut_point)),
                               "",
                               range(cut_point, na.rm = TRUE) %>%
                                 unique() %>%
                                 paste(collapse = "-"))) %>%
  ungroup()


#find AUROC value for selected model
if("model"%in% names(AUROC_data)){
  AUROC_lab <- paste("AUROC =", round(AUROC_data[AUROC_data$model == unique(df$model), ]$auc, 3))
} else { #some changes in next chnk of code made this necessary, col model will be split and removed
  if(str_split(unique(df$model), "_")[[1]][4] == "rainrh"){
    mod_var <- str_split(unique(df$model), "_")
    mod_var[[1]][4] <- "rainrh"
    implode <- function(..., sep='') {paste(..., collapse=sep)}
    mod_var <-  implode(mod_var[[1]],sep="_")   
  }else{
    mod_var <- str_split(unique(df$model), "_")
    implode <- function(..., sep='') {paste(..., collapse=sep)}
    mod_var <-  implode(mod_var[[1]],sep="_") 
  }
  auc_val <- 
    unite(AUROC_data, col = model,  colnames(AUROC_data[,1:4]), sep = "_") %>% 
    filter(model == mod_var) %>% 
    select(auc)
  AUROC_lab <- paste("AUROC =", round(auc_val, 3))
  
}

#Print title without or with lettering (for later analysis)
pars <- str_split(df[1,"model"], "_")
title <- 
  paste0( ifelse(is.null(numbering),"",  paste0(letters[numbering],") ")), 
          "ROC for ", 
          "Tt = ", pars[[1]][[2]],"°C; ",
          "RHt = ", pars[[1]][1], "%; ",
          "SDt = ", pars[[1]][3], 
          "\n",
          " hours; and LW = rain > 0.1mm and RH>90%.")


ggplot(df, aes(one_min_spec, sens, label = cut_point)) +
  geom_abline(
    intercept = 0,
    slope = 1,
    color = "black",
    linetype = "dashed"
  ) +
  geom_path(colour = "gray") +
  geom_point(colour = "black") +
  geom_text_repel(size = 3) +
  scale_y_continuous(limits = c(0, 1),
                     expand = c(0, 0),
                     breaks = c(0, 0.1, 0.25, 0.5, 0.75, 0.9, 1),
                     name = "Sensitivity") +
  scale_x_continuous(limits = c(0, 1),
                     expand = c(0, 0),
                     breaks = c(0, 0.1, 0.25, 0.5, 0.75, 0.9, 1),
                     name = "1- Specificity") +
  ggtitle(title) +
  annotate(
    "text",
    x = 0.7,
    y = 0.15,
    label = AUROC_lab,
    size = 5
  ) +
  theme_bw() +
  theme(
    text = element_text(size = 10.5),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )
}
PlotROC(ROC_data[["90_10_12_rain"]])
```

Prepare the data for further analysis and check the resulting data frame.

```{r parameters}
params <- colnames(parameters[, names(parameters) != "model"])
AUROC_data <-
  separate(AUROC_data, model, into =  params, sep = "_")
# AUROC_data[AUROC_data$lw_rh == "r+rh", "lw_rh"] <- "rainrh"
AUROC_data[, 1:3] <-
  lapply(AUROC_data[, 1:3], as.numeric)
data <- data.frame(AUROC_data)
head(data) %>% gt()
```


# Parameter evaluation

## Leaf wetness estimation

Differences between pairs of models with same parameters, differing only in leaf wetness estimation. We shall consult visual aids to assess if distribution of differences follows normal distribution.

```{r leaf-wet-graphs, fig.align='center'}
t_data <-
  data %>%
  group_by(rh_thresh, temp_thresh, hours) %>%
  unite(var, rh_thresh, temp_thresh, hours) %>%
  spread(key = lw_rh, value = auc) %>%
  mutate(difference = rain - rainrh)
ggplot(t_data, aes(x = difference)) +
  geom_density(fill = "royalblue",
               alpha = 0.5,
               color = NA) +
  geom_point(aes(y = 0),
             alpha = 0.5) +
    geom_histogram(binwidth = 0.001) +
  theme_article()
```

Compute summary statistics by groups.
```{r leaf-wet summary}
group_by(data, lw_rh) %>%
  summarise(
    count = n(),
    median = median(auc, na.rm = TRUE),
    IQR = IQR(auc, na.rm = TRUE)
  ) %>% gt()
```


```{r leaf-wet-shapiro}
shapiro.test(t_data$difference)
```
The p-value < 0.05 implying that the distribution of the data is significantly different from normal distribution.
Use paired-sample Wilcoxon test to determine if median AUROC with r as lw estimator is as good as the median auc with both r and rh as estimators. 

```{r wilcoxon-test}
w.test <- with(t_data,
               wilcox.test(
                 rain,
                 rainrh,
                 paired = TRUE,
                 exact = F,
                 alternative = "less"
               ))
w.test
```

Wilcoxon signed rank test showed that median AUROC is greater when using both rain >0.1 mm and RH>90% instead only rain >0.1 mm and as leaf wetness indicators with p `r {format.pval(w.test$p.value, eps = .001)}`.
Model outputs with rain as single predictor are removed from further analysis.

```{r plot_lw_estim}
plot <- 
data %>%
  mutate(Leaf_Wetness = factor(ifelse(
    lw_rh == "rainrh", "rain and rh", "only rain"
  ))) %>%
  ggplot(aes(
    x = factor(lw_rh),
    y = auc,
    group = Leaf_Wetness
  )) +
  geom_boxplot(aes(colour = Leaf_Wetness), width = 0.4) +
  # geom_label(aes_string(x=1.5, y=height, label=paste("p =",w.test$p.value)), label.size =0.02 )+
  scale_color_discrete(name = "Leaf Wetness",
                       labels = c("Rain>0.01", "Rain>0.01&RH>90%")) +
  scale_x_discrete(breaks = c(0, 90),
                   labels = c("rain", "rh and rain")) +
  xlab("Infection period switch") +
  ylab("AUROC") +
  theme_article() +
  coord_equal(10 / 1)
plot+ggsave(here("data", "op_2007_16","results","LWswithc.png"), width = 11, height = 11, units = "cm")

```

```{r remove-single-rain}
#Remove the data containg only rain as estimator
data <- subset(data, lw_rh == "rainrh")
data$lw_rh <- NULL
```

## T, RH and Sporulation duration

### Initial exploration

Scatter plot matrix shows some relationships between y and other variables.

```{r interactions, fig.align='center'}
GGally::ggpairs(data,
                lower = list(continuous = "points"),
                upper = list(continuous = "cor"))
```

Increasing temperature threshold had positive, while reducing rh and duration of sporulation threshold had negative correlation with AUROC. 

Take initial look at descriptive statistics and trend of AUROC response as a factor of each variable investigated.

```{r desc_stat, fig.align='center', fig.show = "hold",fig.width=6, fig.height=4}
p1 <-  
  data[order(data$hours), ] %>%
  ggplot(., aes(factor(hours), auc)) +
  geom_boxplot(width = 0.4) +
  geom_jitter(
    position = position_jitter(width = 0.2),
    colour = "black",
    alpha = 0.6,
    size = 0.7
  ) +
  ggtitle("Durations of sporulation period") +
  geom_smooth(method = "loess",
              se = T,
              color = "red",
              aes(group = 1)) +
  xlab("Sporulation period (hours)") +
  ylab("AUROC") +
  theme_article()

p2 <- 
  ggplot(data, aes(factor(rh_thresh), auc)) +
  geom_boxplot(width = 0.4) +
  geom_jitter(
    position = position_jitter(width = 0.2),
    colour = "black",
    alpha = 0.6,
    size = 0.7
  ) +
  ggtitle("RH thresholds for sporulation and infection") +
  geom_smooth(method = "loess",
              se = T,
              color = "blue",
              aes(group = 1)) +
  ylab("AUROC") +
  xlab("RH threshold (%)") +
  theme_article()

p3 <-  
  ggplot(data, aes(factor(temp_thresh), auc)) +
  geom_boxplot(width = 0.4) +
  geom_jitter(
    position = position_jitter(width = 0.2),
    colour = "black",
    alpha = 0.6,
    size = 0.7
  ) +
  ggtitle("Temperature thresholds for sporulation and infection") +
  geom_smooth(
    method = "loess",
    se = T,
    color = "black",
    aes(group = 1),
    show.legend = T
  ) +
  xlab("Temperature threshold (°C)") +
  ylab("AUROC") +
  theme_article()

png(filename = here("data", "op_2007_16","results","init_vis.png"),
    width = 400, height = 800,bg = "white", res = NA, restoreConsole = TRUE)
egg::ggarrange(p1,p2,p3, ncol = 1)
dev.off()
p1;p2;p3

```


### Model fitting 

```{r mod_fit_data}
lapply(data[, 1:3], function(x)
  sort(unique(x)))
```
Create coded data set. Seven levels of each variable are used. 

```{r code_data}
cd_data <- coded.data(data, Tt ~  (temp_thresh - 10),
                      RHt ~ (rh_thresh - 90),
                      SDt ~ (hours - 12))
str(cd_data)
head(cd_data) %>% kable()
```

Fit models of first to fourth order, and evaluate fits.

```{r model_fit}
poly_1_fit <-  lm(auc ~ poly(RHt, SDt, Tt, degree = 1), data = cd_data)
poly_2_fit <-  lm(auc ~ poly(RHt, SDt, Tt, degree = 2), data = cd_data)
poly_3_fit <-  lm(auc ~ poly(RHt, SDt, Tt, degree = 3), data = cd_data)
poly_4_fit <-  lm(auc ~ poly(RHt, SDt, Tt, degree = 4), data = cd_data)

compareLM(poly_1_fit,poly_2_fit,poly_3_fit,poly_4_fit)[[2]] %>% 
  add_column( Order = 1:4, .before = 1) %>% 
  gt() 
```

Cubic model seems to be the best fit for our purpose. We want to get the best possible fit to the data, to understand underlying relationships between variable thresholds of our model. Hence, our priority is increase in explanatory power, for which our main guide is adjusted R^2^, while the distriburion of residuals is still fulfilling the assumption of normality. Model of 4^th^ order seems to provide very little gain in terms of fit Adj. R^2^ comparing to 3^rd^ order, and most importantly is not overfitting indicated by Wilks-Shapiro test for model of 4^th^ order. Information crterions are indication high values for all fits because of structure of data set, and this information is disregarded because the model is used for interpretation of the data. 



```{r summary_table2, warning=FALSE }

library(pander)
panderOptions('round', 2)
panderOptions('keep.trailing.zeros', TRUE)
pander(poly_3_fit,add.significance.stars = T)
```


```{r include=FALSE}
rm(poly_1_fit,poly_2_fit,poly_4_fit)
```

Evaluate model fit with diagnostic plot. 

```{r diag_plot,out.width = '50%',fig.show = "hold"}
poly_3_fit$studres <- rstudent(poly_3_fit)
plot(poly_3_fit$studres, main="Residuals vs Order of data")
abline(h = 0, col = "red")
hist(resid(poly_3_fit)) #distriburion of residuals should be approximately  normal
```

Assumptions of normapity of residuals are fulfilled. 

Extract the model formula. The code has been borrowed from [this StackOverflow thread](https://stackoverflow.com/questions/50116648/how-do-i-convert-the-following-poly-output-to-a-function-useable-in-excel)

```{r equation}
processPolyNames = function(coef){
  members = strsplit(mgsub::mgsub(coef,c("poly\\(",", degre.*"),c("","")),", ")[[1]]
  degree = as.numeric(strsplit(strsplit(coef,")")[[1]][2],"\\.")[[1]])
  coef_out = ""
  for(d in seq_along(degree)){
    if(degree[d] == 0) next
    if(degree[d] == 1){
      if(coef_out == ""){
        coef_out = members[d]
      } else {
        coef_out = paste0(coef_out,"*",members[d])
      }
    } else {
      if(coef_out == ""){
        coef_out = paste0(members[d],"^",degree[d],"^")
      } else {
        coef_out = paste0(coef_out,"*",members[d],"^",degree[d],"^")
      }
    }
  }
  return(coef_out)
}

coefs = summary(poly_3_fit)$coef[,1]
prettyNames = lapply(names(coefs)[-1],processPolyNames)
prettyModel = ""
for(i in seq_along(coefs)){
  if(i == 1){
    prettyModel = paste0(prettyModel,round(coefs[i],2))
  } else {
    prettyModel = paste0(prettyModel,ifelse(coefs[i] >= 0," + "," "),round(coefs[i],2),"*",prettyNames[[i-1]])
  }
}
prettyModel <-  paste("AUROC =", gsub("-", "- ", prettyModel))
cat(prettyModel);rm(processPolyNames,coefs,prettyNames)

```
`r format(prettyModel)`

```{r rm_formula,include=FALSE}
rm(prettyModel)
```

### Surface plot of model fits 

Visualise fitted response surface. 

```{r persp, fig.show = "hold", warning=FALSE, message=FALSE,fig.width=10, fig.height=10, fig.path='figures/', dev=c('png', 'pdf')}
z_min <- min(data$auc) - c(min(data$auc) * 0.02)
z_max <- max(data$auc) + c(max(data$auc) * 0.02)

# use a viridis palette for usability
library(viridis)
color_var <- plasma(256)

par(mar = c(1.5, 4.5, 3.5, 1.5),
    mfrow = c(3, 3),
    cex = 0.5)

par_cut <- data.frame(
  Tt = c(rep(0, 6), -3, 0,+3),
  RHt = c(rep(0, 3), -3, 0,+3, rep(0, 3)),
  SDt = c(-3, 0,+3, rep(0, 6))
)

plot_ls <- list()

for (i in seq(nrow(par_cut))) {
  persp(poly_3_fit,
        if (i <= 3) {
          ~ RHt + Tt
        } else if (i %in% 4:6) {
          ~ SDt + Tt
        } else if (i >= 7) {
          ~ SDt + RHt
        }
        ,
        at = data.frame(par_cut[i,]),
    zlab = "\nAUROC",
    xlab = if (i <= 3) {
          c("RHt (%)","Tt (°C)")
        } else if (i %in% 4:6) {
          c("SDt (hours)", "Tt (°C)")
        } else if (i >= 7) {
          c("SDt (hours)", "RHt (%)")
        },
    col = color_var,
    zlim = c(z_min, z_max),
    theta = 37,
    phi = 10,
    border = "grey15",
    lwd = 0.5,
    cex.main = 1.8,
    cex.axis = 1.06,
    cex.lab = 1.8,
    main = paste0("\n",letters[i],") ", "Response at ", 
                  "Tt = ", par_cut[i,1] + 10,"°C;",
                   "\n",
                  "RHt = ", par_cut[i,2] + 90, "%; ",
                  "SDt = ", par_cut[i,3] +12, " hours."
                  )
  )
}
```

Plots show relationship between variation in parameters of two variables with third variable fixed at three levels and response as AUROC. 3D surface plots indicate that reduced sporulation duration and relative humidity threshold improves accuracy of the model; while model versions with increased teperature threshold have better diagnostic performance. 

Another way to present this data is with countour plots. We will plot contour plots corresponding to the above 3D surfaces 

```{r countour, fig.show = "hold", warning=FALSE, message=FALSE,fig.width=5, fig.height=10, fig.path='figures/', dev=c('png', 'pdf')}

par(
  mar = c(3, 4, 4, 2),
    mfrow = c(3, 1),
    cex.main = 1.2,
    cex.axis = 1.06,
    cex.lab = 1.1)
# c(5, 4, 4, 2)

contour(poly_3_fit, ~ Tt + RHt, image=TRUE, at = data.frame(Tt = 0, RHt = 0, SDt = -3 ))
# contour(poly_3_fit, ~ Tt + RHt, image=TRUE, at = data.frame(Tt = 0, RHt = 0, SDt = 0))
# contour(poly_3_fit, ~ Tt + RHt, image=TRUE, at = data.frame(Tt = 0, RHt = 0, SDt =  +3))

contour(poly_3_fit, ~ Tt + SDt, image=TRUE, at = data.frame(Tt = 0, RHt = -3, SDt = 0 ))
# contour(poly_3_fit, ~ Tt + SDt, image=TRUE, at = data.frame(Tt = 0, RHt = 0, SDt = 0 ))
# contour(poly_3_fit, ~ Tt + SDt, image=TRUE, at = data.frame(Tt = 0, RHt = 3, SDt = 0 ))

# contour(poly_3_fit, ~ SDt + RHt, image=TRUE, at = data.frame(Tt = -3, RHt = 0, SDt = 0 ))
# contour(poly_3_fit, ~ SDt + RHt, image=TRUE, at = data.frame(Tt = 0, RHt = 0, SDt = 0 ))
contour(poly_3_fit, ~ SDt + RHt, image=TRUE, at = data.frame(Tt = 3, RHt = 0, SDt = 0 ))

```


### Further investigation

Based on the sensitivity analysis followig model parametrisations were compared to the original parameters of Irish Rules and further analised: 

```{r further_analysis, fig.show = "hold", warning=FALSE, fig.width=12, fig.height=9,message=FALSE, fig.path='figures/', dev=c('png', 'pdf')}
# fig.width=5, fig.height=5,
# par(
#     mfrow = c(1, 4)
#   )

mod_list <-  list(
                  ROC_data[["88_10_12_rainrh"]],
                  ROC_data[["90_10_10_rainrh"]],
                  ROC_data[["90_12_12_rainrh"]],
                  ROC_data[["90_10_12_rain"]],
                  ROC_data[["88_12_10_rainrh"]],
                  ROC_data[["88_10_10_rainrh"]])

pl <- map2(mod_list, c(1:length(mod_list)), PlotROC)
cowplot::plot_grid(pl[[1]], 
                   pl[[2]], 
                   pl[[3]],
                   pl[[4]], 
                   pl[[5]], 
                   pl[[6]], 
                   ncol=3)
```



```{r plot-ROC-figures, fig.align='center', fig.width=7, fig.height=7, warning=FALSE,fig.path='figures/', dev=c('png', 'pdf')}

ls_roc <- lapply(mod_list[4:6], function(x) {
  df <- x[rev(x$cut_point),]
  #append rows for plotting
  x <- rep(NA, ncol(df))
  df <- rbind(x, df)
  df[nrow(df) + 1,] <- NA
  df$model <- unique(df$model[!is.na(df$model)])
  df[1,c("sens","one_min_spec")] <- 0
  df[nrow(df),c("sens","one_min_spec")] <- 1
  #Condense labels for a single cutoff point
 df <- 
  df %>% 
  group_by(one_min_spec, sens, model) %>% 
  summarise(cut_point = ifelse(all(is.na(cut_point)),
                               "",
                               range(cut_point, na.rm = TRUE) %>%
                                 unique() %>%
                                 paste(collapse = "-"))) %>%
  ungroup()
  return(df)
})
df <- bind_rows(ls_roc)

rename(df, Model = model) %>%
  ggplot(aes(
    one_min_spec,
    sens,
    group = Model,
    color = Model,
    label = cut_point
  )) +
  # geom_abline(intercept=0, slope = 1, color="black", linetype="dashed")+
  geom_point() +
  geom_text_repel(size = 2) +
  geom_path(colour = "gray") +
  geom_line() +
  scale_y_continuous(limits = c(0, 1),
                     expand = c(0, 0),
                     name = "Sensitivity") +
  scale_x_continuous(limits = c(0, 1),
                     expand = c(0, 0),
                     name = "1- Specificity") +
  theme_minimal() +
  theme(
    text = element_text(size = 14),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(
      linetype = "dashed",
      size = 0.1,
      fill = "NA",
      colour = "black"
    ),
    legend.position = c(.95, .15),
    legend.justification = c("right", "bottom"),
    # legend.box.just = "right",
    legend.margin = margin(6, 6, 6, 6)
  )+
  ggtitle("ROC for selected models")
```

IR have failed to indicate any risk in 2  and the warning threshold of 12 hour was reached in only 4 years out of 10. 




Packages used
```{r }
sessionInfo()
```
